{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IvanMadman/Gemma7b-llama3.8b-gemma2.9b.colab/blob/main/LLM_Gemma7b_Gemma2_9b_Llama3_8b_IT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INFORMATION!**\n",
        "\n",
        "This is a very simple notebook created to test in a colab hosted environment 3 of the most famous LLMs in their small Instruct version, it's a very basic implementation but it's CUDA enabled and should be fast enough for our needs.\n",
        "It's using llama cpp python and a gradio UI.\n",
        "\n",
        "Be aware that i'm using the standard high level API for testing purposes (using \"prompt\" input instead of \"messages[]\") but the chat_completions method it's the suggested one.\n",
        "\n",
        "**HOW IT WORKS**\n",
        "\n",
        "**Before running anything, make sure your runtime it's GPU enabled or CUDA installation will fail!!**\n",
        "\n",
        "Just run the first cell and let it install everything, when it's done you can run the second cell and after 2-3 minutes you should have your gladio link to access the UI. You can choose which one of the models you wanna use and click on \"load model\", it will automatically download it (you can look at the running cell to see the estimated time left). When you will want to change the model it will check for storage space and delete the other models if it's needed, unload the current model to free up memory, download and load the new one."
      ],
      "metadata": {
        "id": "KQPyg03MIBbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python \\\n",
        "  --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!pip install torch==2.3.0 torchvision==0.18.0 torchaudio==2.3.0 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install gradio huggingface_hub"
      ],
      "metadata": {
        "id": "nST5uNboi8Q-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge0k_BiQZfVE"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from llama_cpp import Llama\n",
        "import os\n",
        "from huggingface_hub import hf_hub_download\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import shutil\n",
        "import gc\n",
        "\n",
        "# Updated dictionary of model information\n",
        "model_info = {\n",
        "    \"Gemma 7B\": {\n",
        "        \"repo_id\": \"mlabonne/gemma-7b-it-GGUF\",\n",
        "        \"filename\": \"gemma-7b-it.Q5_K_M.gguf\",\n",
        "        \"settings\": {\n",
        "            \"n_ctx\": 8192,\n",
        "            \"n_batch\": 1024,\n",
        "            \"rope_scaling_type\": 1,\n",
        "            \"rope_freq_base\": 10000,\n",
        "            \"rope_freq_scale\": 1.0,\n",
        "        }\n",
        "    },\n",
        "    \"Llama 3 8B Instruct\": {\n",
        "        \"repo_id\": \"bartowski/Llama-3-Instruct-8B-SPPO-Iter3-GGUF\",\n",
        "        \"filename\": \"Llama-3-Instruct-8B-SPPO-Iter3-Q6_K.gguf\",\n",
        "        \"settings\": {\n",
        "            \"n_ctx\": 4096,\n",
        "            \"n_batch\": 512,\n",
        "            \"rope_scaling_type\": 0,\n",
        "            \"rope_freq_base\": 10000,\n",
        "            \"rope_freq_scale\": 1.0,\n",
        "        }\n",
        "    },\n",
        "    \"Gemma 2 9B\": {\n",
        "        \"repo_id\": \"bartowski/gemma-2-9b-it-GGUF\",\n",
        "        \"filename\": \"gemma-2-9b-it-Q5_K_M.gguf\",\n",
        "        \"settings\": {\n",
        "            \"n_ctx\": 8192,\n",
        "            \"n_batch\": 512,\n",
        "            \"rope_scaling_type\": 1,\n",
        "            \"rope_freq_base\": 10000,\n",
        "            \"rope_freq_scale\": 1.0,\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to download models (unchanged)\n",
        "def download_model(repo_id, filename):\n",
        "    try:\n",
        "        model_path = hf_hub_download(repo_id=repo_id, filename=filename, resume_download=True)\n",
        "        print(f\"Download completed: {model_path}\")\n",
        "        return model_path\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading model: {e}\")\n",
        "        return None\n",
        "\n",
        "# Global variable to store the current model\n",
        "current_model = None\n",
        "system_prompt = None\n",
        "# Function to check available disk space (unchanged)\n",
        "def get_free_space(path):\n",
        "    stat = shutil.disk_usage(path)\n",
        "    return stat.free // (2**30)  # Convert bytes to GB\n",
        "\n",
        "# Function to clear the downloaded model folder\n",
        "def clear_model_folder():\n",
        "    cache_dir = os.path.expanduser(\"~/.cache/huggingface/hub\")\n",
        "    if os.path.exists(cache_dir):\n",
        "        shutil.rmtree(cache_dir)\n",
        "        print(\"Model folder cleared.\")\n",
        "    else:\n",
        "        print(\"Model folder not found.\")\n",
        "\n",
        "# Function to unload the current model and clear GPU memory\n",
        "def unload_model():\n",
        "    global current_model\n",
        "    if current_model is not None:\n",
        "        del current_model\n",
        "        current_model = None\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    print(\"Model unloaded and GPU memory cleared.\")\n",
        "\n",
        "# Function to load a model\n",
        "def load_model(model_name):\n",
        "    global current_model\n",
        "\n",
        "    # Unload the current model if one is loaded\n",
        "    unload_model()\n",
        "\n",
        "    model_info_dict = model_info.get(model_name)\n",
        "    if not model_info_dict:\n",
        "        return f\"Model {model_name} not found in the model_info dictionary.\"\n",
        "\n",
        "    repo_id = model_info_dict['repo_id']\n",
        "    filename = model_info_dict['filename']\n",
        "    settings = model_info_dict['settings']\n",
        "\n",
        "    # Check available space\n",
        "    free_space = get_free_space(\".\")\n",
        "    if free_space < 10:  # Assuming we need at least 10GB free space\n",
        "        user_response = input(f\"Low disk space ({free_space}GB available). Clear model folder? (y/n): \")\n",
        "        if user_response.lower() == 'y':\n",
        "            clear_model_folder()\n",
        "        else:\n",
        "            return \"Operation cancelled due to low disk space.\"\n",
        "\n",
        "    print(f\"Downloading {model_name}...\")\n",
        "    model_path = download_model(repo_id, filename)\n",
        "\n",
        "    if model_path:\n",
        "        print(f\"Loading {model_name}...\")\n",
        "        try:\n",
        "            current_model = Llama(\n",
        "                model_path=model_path,\n",
        "                n_gpu_layers=-1,  # Use all available GPU layers\n",
        "                n_ctx=settings['n_ctx'],\n",
        "                max_tokens=settings['n_ctx'],\n",
        "                offload_kqv=True,\n",
        "                f16_kv=True,\n",
        "                use_mlock=False,\n",
        "                use_mmap=True,\n",
        "                embedding_mode=\"llama\",\n",
        "                n_threads=os.cpu_count(),\n",
        "                n_batch=settings['n_batch'],\n",
        "                use_parallel_residual=True,\n",
        "                verbose=True,\n",
        "                tensor_split=None,\n",
        "                rope_scaling_type=settings['rope_scaling_type'],\n",
        "                rope_freq_base=settings['rope_freq_base'],\n",
        "                rope_freq_scale=settings['rope_freq_scale']\n",
        "            )\n",
        "            print(f\"{model_name} loaded successfully!\")\n",
        "            return f\"{model_name} loaded successfully with model-specific optimizations!\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model: {e}\")\n",
        "            return f\"Failed to load {model_name}. Error: {str(e)}\"\n",
        "    else:\n",
        "        return f\"Failed to download {model_name}.\"\n",
        "\n",
        "\n",
        "\n",
        "# Updated function to generate a response\n",
        "def generate_response(message, history, max_tokens, model_name):\n",
        "    print(history)\n",
        "    current_model.reset()\n",
        "    global system_prompt\n",
        "    # Start with an empty prompt\n",
        "    prompt=\"\"\n",
        "    if current_model is None:\n",
        "        return \"\", history + [(\"You\", message), (\"Assistant\", \"Please select and load a model first.\")]\n",
        "\n",
        "    # Prepare the prompt based on the selected model\n",
        "    if model_name in [\"Gemma 7B\", \"Gemma 2 9B\"]:\n",
        "        for role, content in history:\n",
        "            prompt += f\"<start_of_turn>{role.lower()}\\n{content}\\n<end_of_turn>\\n\"\n",
        "        prompt += \"<start_of_turn>user\\n\" + message + \"\\n<end_of_turn>\\n<start_of_turn>model\\n\"\n",
        "        stop = [\"<end_of_turn>\", \"<start_of_turn>\"]\n",
        "        print(prompt)\n",
        "\n",
        "    else:  # Llama 3 8B Instruct\n",
        "        if not history:\n",
        "            system_prompt = \"You are a helpful AI assistant. Answer the user's questions to the best of your ability.\"\n",
        "            prompt += f\"<|start_header_id|>system<|end_header_id|>\\n\\n{system_prompt}<|eot_id|>\\n\\n\"\n",
        "        for role, content in history:\n",
        "            prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}<|eot_id|>\\n\\n\"\n",
        "        prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{message}<|eot_id|>\\n\\n\"\n",
        "        prompt += f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
        "        print(prompt)\n",
        "        stop = [\"<|eot_id|>\", \"<|start_header_id|>\"]\n",
        "\n",
        "\n",
        "    # Generate response\n",
        "    try:\n",
        "        response = current_model(\n",
        "            prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            stop=stop,\n",
        "            echo=False,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            repeat_penalty=1.1,\n",
        "            top_k=40\n",
        "        )\n",
        "        torch.cuda.empty_cache()\n",
        "        print(response)\n",
        "        ai_message = response['choices'][0]['text'].strip()\n",
        "        if model_name in [\"Gemma 7B\", \"Gemma 2 9B\"]:\n",
        "            return \"\", history + [(\"user\", message), (\"model\", ai_message)]\n",
        "        else:\n",
        "            return \"\", history + [(\"user\", message), (\"assistant\", ai_message)]\n",
        "    except Exception as e:\n",
        "        error_message = f\"An error occurred while generating the response: {str(e)}\"\n",
        "        return error_message, history + [(\"user\", message), (\"assistant\", error_message)]\n",
        "\n",
        "# Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# LLM Chat Interface (GPU Accelerated)\")\n",
        "\n",
        "    with gr.Row():\n",
        "        model_dropdown = gr.Dropdown(choices=list(model_info.keys()), label=\"Select Model\")\n",
        "        load_button = gr.Button(\"Load Model\")\n",
        "\n",
        "    with gr.Row():\n",
        "        max_tokens_slider = gr.Slider(10, 8192, step=10, value=512, label=\"Max Tokens\")\n",
        "\n",
        "    chatbot = gr.Chatbot(render_markdown=True)\n",
        "    msg = gr.Textbox(label=\"Your message\")\n",
        "    clear = gr.Button(\"Clear Chat\")\n",
        "\n",
        "    load_status = gr.Textbox(label=\"Model Status\")\n",
        "\n",
        "    def load_model_wrapper(model_name):\n",
        "        return load_model(model_name)\n",
        "\n",
        "    load_button.click(load_model_wrapper, inputs=[model_dropdown], outputs=[load_status])\n",
        "    msg.submit(generate_response, inputs=[msg, chatbot, max_tokens_slider, model_dropdown], outputs=[msg, chatbot])\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "# Check for CUDA availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# Launch the interface\n",
        "demo.queue().launch(share=True, debug=True, inline=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_sfAj5CsRe8F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}